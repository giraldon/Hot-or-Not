{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Million Song Baseline #\n",
    "\n",
    "Baseline model to get a feel for loading the data + setting up logistic regression on the million song subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tables\n",
    "import seaborn as sns\n",
    "#import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "import imblearn.over_sampling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) \\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "``E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\MillionSongSubset\\data\\train.h5`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-75cefbea8b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\MillionSongSubset\\data\\train.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m# Finally, create the File instance, and return it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mcheck_file_access\u001b[0;34m(filename, mode)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# The file should be readable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` does not exist\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"``%s`` is not a regular file\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ``E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\MillionSongSubset\\data\\train.h5`` does not exist"
     ]
    }
   ],
   "source": [
    "train = tables.open_file(r'E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\MillionSongSubset\\data\\train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_metadata = train.root.metadata.songs\n",
    "song_metadata_df = pd.DataFrame.from_records(song_metadata[:])\n",
    "song_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_analysis = train.root.analysis.songs\n",
    "song_analysis_df = pd.DataFrame.from_records(song_analysis[:])\n",
    "song_analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data = song_metadata_df.merge(song_analysis_df, on=song_analysis_df.index)\n",
    "full_song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data.idx_artist_terms[221]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data = full_song_data.drop(['key_0', 'analyzer_version', 'artist_7digitalid', 'artist_playmeid',\\\n",
    "                                      'idx_artist_terms', 'idx_similar_artists', 'release_7digitalid', 'analysis_sample_rate',\\\n",
    "                                      'idx_bars_confidence', 'idx_bars_start', 'idx_beats_confidence', 'idx_beats_start',\\\n",
    "                                      'idx_sections_confidence', 'idx_sections_start', 'idx_segments_confidence',\\\n",
    "                                      'idx_segments_loudness_max', 'idx_segments_loudness_max_time', 'idx_segments_loudness_start',\\\n",
    "                                      'idx_segments_pitches', 'idx_segments_start', 'idx_segments_timbre', 'idx_tatums_confidence',\\\n",
    "                                      'idx_tatums_start', 'track_7digitalid', 'audio_md5', 'artist_id', 'artist_latitude',\\\n",
    "                                      'artist_longitude', 'artist_mbid', 'song_id', 'song_hotttnesss', 'artist_location', 'genre'], axis=1)\n",
    "full_song_data = full_song_data.sort_values(by='danceability', ascending=False)\n",
    "full_song_data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_song_data[full_song_data['danceability'] == 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_song_data.key[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_b(value):\n",
    "    if type(value) == bytes:\n",
    "        no_b = str(value).lstrip('b').replace('&', 'and')\n",
    "        no_quote = no_b.replace('\"', '')\n",
    "        return no_quote.replace(\"'\", \"\").lower()\n",
    "    elif type(value) == float and value == 0.0:\n",
    "        return np.nan\n",
    "    else: \n",
    "        return value\n",
    "\n",
    "full_song_data = full_song_data.applymap(clean_b)\n",
    "full_song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = pd.read_csv(r'E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\MillionSongSubset\\AdditionalFiles\\subset_tracks_per_year.txt', sep='<SEP>', names=['year', 'track_id', 'artist_name', 'title'])\n",
    "year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(value):\n",
    "    return value.lower()\n",
    "year_only = year.drop(['artist_name', 'title'], axis = 1)\n",
    "year_only['track_id'] = year_only['track_id'].astype(str).apply(lowercase)\n",
    "full_song_data = full_song_data.merge(year_only, on='track_id')\n",
    "full_song_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_only['year'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data = full_song_data.drop(['danceability', 'energy'], axis=1)\n",
    "full_song_data = full_song_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data.shape\n",
    "#hot_100['title'] = hot_100['song']\n",
    "#hot_100['artist_name'] = hot_100['performer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_100 = pd.read_csv(r'E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\get-top-40-hot-or-not-QueryResult.csv')\n",
    "hot_100 = hot_100.rename(index=str, columns={\"song\": \"title\", \"performer\": \"artist_name\"})\n",
    "hot_100['isin_hot100'] = 1\n",
    "hot_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ands(value):\n",
    "    value = str(value).replace('\"', '')\n",
    "    value = value.replace(\"'\", \"\")\n",
    "    return value.replace('&', 'and').lower()\n",
    "hot_100 = hot_100.applymap(clean_ands)\n",
    "hot_100.sort_values('title').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_song_data.sort_values('title').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_song_data = full_song_data.merge(hot_100, how='left', on=['title', 'artist_name'])\n",
    "merged_song_data = merged_song_data.sort_values(by='year', ascending=True)\n",
    "merged_song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_song_data['isin_hot100']=merged_song_data['isin_hot100'].fillna(0)\n",
    "merged_song_data['isin_hot100']=merged_song_data['isin_hot100'].apply(pd.to_numeric)\n",
    "merged_song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_song_data_clean = merged_song_data.drop(['key_confidence', 'mode_confidence', 'time_signature_confidence', 'track_id'], axis=1)\n",
    "merged_song_data_clean['time_signature'] = merged_song_data_clean['time_signature'].astype(str)\n",
    "merged_song_data_clean['key'] = merged_song_data_clean['key'].astype(str)\n",
    "merged_song_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(merged_song_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_song_data_clean.to_csv(r'E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\MillionSongSubset\\data\\merged_song_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_song_data_clean['year'] = merged_song_data_clean['year'] - int(1927)\n",
    "merged_song_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's do the same on the holdout set ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout = tables.open_file(r'E:\\Nicolas\\Documents\\Metis\\Music_Baseline\\MillionSongSubset\\data\\test.h5')\n",
    "holdout_metadata = holdout.root.metadata.songs\n",
    "holdout_metadata_df = pd.DataFrame.from_records(holdout_metadata[:])\n",
    "holdout_analysis = holdout.root.analysis.songs\n",
    "holdout_analysis_df = pd.DataFrame.from_records(holdout_analysis[:])\n",
    "full_holdout_data = holdout_metadata_df.merge(holdout_analysis_df, on=holdout_analysis_df.index)\n",
    "full_holdout_data = full_holdout_data.drop(['key_0', 'analyzer_version', 'artist_7digitalid', 'artist_playmeid',\\\n",
    "                                      'idx_artist_terms', 'idx_similar_artists', 'release_7digitalid', 'analysis_sample_rate',\\\n",
    "                                      'idx_bars_confidence', 'idx_bars_start', 'idx_beats_confidence', 'idx_beats_start',\\\n",
    "                                      'idx_sections_confidence', 'idx_sections_start', 'idx_segments_confidence',\\\n",
    "                                      'idx_segments_loudness_max', 'idx_segments_loudness_max_time', 'idx_segments_loudness_start',\\\n",
    "                                      'idx_segments_pitches', 'idx_segments_start', 'idx_segments_timbre', 'idx_tatums_confidence',\\\n",
    "                                      'idx_tatums_start', 'track_7digitalid', 'audio_md5', 'artist_id', 'artist_latitude',\\\n",
    "                                      'artist_longitude', 'artist_mbid', 'song_id', 'song_hotttnesss', 'artist_location', 'genre',\\\n",
    "                                      'key_confidence', 'mode_confidence', 'time_signature_confidence', 'track_id', 'danceability', 'energy'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = merged_song_data_clean.loc[:,['artist_name', 'title', 'artist_familiarity', 'artist_hotttnesss', 'duration',\\\n",
    "                                       'end_of_fade_in', 'key', 'loudness', 'mode', 'start_of_fade_out', 'tempo',\\\n",
    "                                       'time_signature', 'year']]\n",
    "y_data = merged_song_data_clean['isin_hot100']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establishing a Baseline with Logistic Regression and SMOTE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, x_test, y, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['artist_hotttnesss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = imblearn.over_sampling.SMOTE(ratio=.4, random_state = 42) \n",
    "    \n",
    "X_tr_smote, y_tr_smote = smote.fit_sample(X.loc[:,selection], y)\n",
    "\n",
    "lr_smote = LogisticRegression() \n",
    "lr_smote.fit(X_tr_smote, y_tr_smote)\n",
    "y_pred = lr_smote.predict(x_test.loc[:,selection])\n",
    "\n",
    "print('Logistic Regression on SMOTE Train Data, ratio 0.4; Test F1: %.3f, Test AUC: %.3f' % \\\n",
    "      (f1_score(lr_smote.predict(x_test.loc[:,selection]), y_test), roc_auc_score(y_test, lr_smote.predict_proba(x_test.loc[:,selection])[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = x_test.loc[:,selection], y_test # explicitly calling this validation since we're using it for selection\n",
    "\n",
    "thresh_ps = np.linspace(.10,.50,1000)\n",
    "model_val_probs = lr_smote.predict_proba(x_val)[:,1] # positive class probs, same basic logistic model we fit in section 2 \n",
    "\n",
    "f1_scores = []\n",
    "for p in thresh_ps:\n",
    "    model_val_labels = model_val_probs >= p\n",
    "    f1_scores.append(f1_score(model_val_labels, y_val))\n",
    "    \n",
    "plt.plot(thresh_ps, f1_scores)\n",
    "plt.title('F1 Score vs. Positive Class Decision Probability Threshold')\n",
    "plt.xlabel('P threshold')\n",
    "plt.ylabel('F1 score')\n",
    "\n",
    "best_f1_score = np.max(f1_scores) \n",
    "best_thresh_p = thresh_ps[np.argmax(f1_scores)]\n",
    "\n",
    "print('Logistic Regression Model best F1 score %.3f at prob decision threshold >= %.3f' \n",
    "      % (best_f1_score, best_thresh_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try some more features ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['artist_familiarity', 'artist_hotttnesss', 'duration', 'key', 'loudness', 'mode', 'tempo', 'time_signature', 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "x_train_cat = pd.get_dummies(X.loc[:,selection], columns=['key', 'time_signature'])\n",
    "x_test_cat = pd.get_dummies(x_test.loc[:,selection], columns=['key', 'time_signature'])\n",
    "x_train_scaled = std.fit_transform(x_train_cat)\n",
    "x_test_scaled = std.fit_transform(x_test_cat)\n",
    "\n",
    "smote = imblearn.over_sampling.SMOTE(ratio=.4, random_state = 42) \n",
    "    \n",
    "X_tr_smote, y_tr_smote = smote.fit_sample(x_train_scaled, y)\n",
    "\n",
    "lr_smote1 = LogisticRegression(class_weight={1 : 4, 0 : 1}) \n",
    "lr_smote2 = LogisticRegression(class_weight='balanced')\n",
    "lr_smote1.fit(X_tr_smote, y_tr_smote)\n",
    "lr_smote2.fit(X_tr_smote, y_tr_smote)\n",
    "y_pred1 = lr_smote1.predict(x_test_scaled)\n",
    "y_pred2 = lr_smote2.predict(x_test_scaled)\n",
    "\n",
    "print('Logistic Regression on SMOTE Train Data, ratio 0.4, weight balanced; Test F1: %.3f, Test AUC: %.3f, RMSE: %.3f'  % \\\n",
    "      (f1_score(y_pred1, y_test), roc_auc_score(y_test, lr_smote1.predict_proba(x_test_scaled)[:,1]), np.sqrt(np.sum((y_pred1 - y_test)**2)/len(y_test))))\n",
    "\n",
    "print('Logistic Regression on SMOTE Train Data, ratio 0.4, weight 3:1; Test F1: %.3f, Test AUC: %.3f, RMSE: %.3f' % \\\n",
    "      (f1_score(y_pred2, y_test), roc_auc_score(y_test, lr_smote2.predict_proba(x_test_scaled)[:,1]), np.sqrt(np.sum((y_pred2 - y_test)**2)/len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = x_test_scaled, y_test # explicitly calling this validation since we're using it for selection\n",
    "\n",
    "thresh_ps = np.linspace(.10,1,1000)\n",
    "model_val_probs = lr_smote1.predict_proba(x_val)[:,1] # positive class probs, same basic logistic model we fit in section 2 \n",
    "\n",
    "f1_scores = []\n",
    "for p in thresh_ps:\n",
    "    model_val_labels = model_val_probs >= p\n",
    "    f1_scores.append(f1_score(model_val_labels, y_val))\n",
    "    \n",
    "plt.plot(thresh_ps, f1_scores)\n",
    "plt.title('F1 Score vs. Positive Class Decision Probability Threshold')\n",
    "plt.xlabel('P threshold')\n",
    "plt.ylabel('F1 score')\n",
    "\n",
    "best_f1_score = np.max(f1_scores) \n",
    "best_thresh_p = thresh_ps[np.argmax(f1_scores)]\n",
    "\n",
    "print('Logistic Regression Model best F1 score %.3f at prob decision threshold >= %.3f' \n",
    "      % (best_f1_score, best_thresh_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators = 1000, max_features = 3,\n",
    "                                min_samples_leaf = 5, n_jobs=-1, class_weight='balanced')\n",
    "rf2 = RandomForestClassifier(n_estimators = 1000, max_features = 5,\n",
    "                                min_samples_leaf = 5, n_jobs=-1, class_weight={1 : 4, 0 : 1})\n",
    "\n",
    "x_smote, y_smote = smote.fit_sample(X.loc[:,selection], y)\n",
    "\n",
    "rf1.fit(X_tr_smote, y_smote)\n",
    "rf2.fit(X_tr_smote, y_smote)\n",
    "y_pred1 = rf1.predict(x_test_scaled)\n",
    "y_pred2 = rf2.predict(x_test_scaled)\n",
    "\n",
    "print('Random Forest on SMOTE Train Data, ratio 0.4; Test F1: %.3f, Test AUC: %.3f' % \\\n",
    "      (f1_score(y_pred1, y_test), roc_auc_score(y_test, rf1.predict_proba(x_test_scaled)[:,1])))\n",
    "print(rf1.feature_importances_)\n",
    "print('Random Forest on SMOTE Train Data, ratio 0.4; Test F1: %.3f, Test AUC: %.3f' % \\\n",
    "      (f1_score(y_pred2, y_test), roc_auc_score(y_test, rf2.predict_proba(x_test_scaled)[:,1])))\n",
    "print(rf2.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = x_test_scaled, y_test # explicitly calling this validation since we're using it for selection\n",
    "\n",
    "thresh_ps = np.linspace(.10,1,1000)\n",
    "model_val_probs = rf2.predict_proba(x_val)[:,1] # positive class probs, same basic logistic model we fit in section 2 \n",
    "\n",
    "f1_scores = []\n",
    "for p in thresh_ps:\n",
    "    model_val_labels = model_val_probs >= p\n",
    "    f1_scores.append(f1_score(model_val_labels, y_val))\n",
    "    \n",
    "plt.plot(thresh_ps, f1_scores)\n",
    "plt.title('F1 Score vs. Positive Class Decision Probability Threshold')\n",
    "plt.xlabel('P threshold')\n",
    "plt.ylabel('F1 score')\n",
    "\n",
    "best_f1_score = np.max(f1_scores) \n",
    "best_thresh_p = thresh_ps[np.argmax(f1_scores)]\n",
    "\n",
    "print('Logistic Regression Model best F1 score %.3f at prob decision threshold >= %.3f' \n",
    "      % (best_f1_score, best_thresh_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1 = xgboost(n_estimators = 1000, learning_rate = 1, max_depth = 7)\n",
    "gb1.fit(x_smote, y_smote)\n",
    "y_pred1 = gb1.predict(x_test.loc[:,selection])\n",
    "\n",
    "print('Gradient Boosting on SMOTE Train Data, ratio 0.4; Test F1: %.3f, Test AUC: %.3f' % \\\n",
    "      (f1_score(y_pred1, y_test), roc_auc_score(y_test, gb1.predict_proba(x_test.loc[:,selection])[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = x_test.loc[:,selection], y_test # explicitly calling this validation since we're using it for selection\n",
    "\n",
    "thresh_ps = np.linspace(.10,0.5,1000)\n",
    "model_val_probs = gb1.predict_proba(x_val)[:,1]\n",
    "f1_scores = []\n",
    "for p in thresh_ps:\n",
    "    model_val_labels = model_val_probs >= p\n",
    "    f1_scores.append(f1_score(model_val_labels, y_val))\n",
    "    \n",
    "plt.plot(thresh_ps, f1_scores)\n",
    "plt.title('F1 Score vs. Positive Class Decision Probability Threshold')\n",
    "plt.xlabel('P threshold')\n",
    "plt.ylabel('F1 score')\n",
    "\n",
    "best_f1_score = np.max(f1_scores) \n",
    "best_thresh_p = thresh_ps[np.argmax(f1_scores)]\n",
    "\n",
    "print('Logistic Regression Model best F1 score %.3f at prob decision threshold >= %.3f' \n",
    "      % (best_f1_score, best_thresh_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1.plot_importance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
